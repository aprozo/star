{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilv6MKqrT9sd",
        "outputId": "97cd5c59-abbc-4d34-b826-aaa6bd29af21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-01 13:37:50.682403: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-02-01 13:37:50.861243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-01 13:37:51.722903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-01 13:37:56.304236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-02-01 13:37:56.417294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-02-01 13:37:56.417353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras import layers, utils, backend, callbacks\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import modplot\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from hep_ml import reweight\n",
        "\n",
        "import uproot\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]) #in MB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ZFit \n",
        "Returns weights for centrality 0, 1 or 2 (0-10,10-40,40-80 %)\n",
        "\n",
        "Important : (D0mass > 1.75) & (D0mass<2.0) cuts should be always applied\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prozorov/install/miniconda3/envs/tf/lib/python3.11/site-packages/zfit/__init__.py:63: UserWarning: TensorFlow warnings are by default suppressed by zfit. In order to show them, set the environment variable ZFIT_DISABLE_TF_WARNINGS=0. In order to suppress the TensorFlow warnings AND this warning, set ZFIT_DISABLE_TF_WARNINGS=1.\n",
            "  warnings.warn(\n",
            "2024-01-31 17:53:39.472973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-31 17:53:39.473064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-31 17:53:39.473086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-31 17:53:49.532505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-31 17:53:49.532962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2024-01-31 17:53:49.533627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-31 17:53:49.534310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-31 17:53:49.538492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "import mplhep\n",
        "import numpy as np\n",
        "import zfit\n",
        "from matplotlib import pyplot as plt\n",
        "import uproot\n",
        "from hepstats.splot import compute_sweights\n",
        "\n",
        "obs = zfit.Space('D0mass', limits=(1.75, 2.0))\n",
        "mu = zfit.Parameter(\"mu\", 1.865, 1.8, 1.92, step_size=0.0001)\n",
        "sigma = zfit.Parameter(\"sigma\", 0.02, 0.001, 0.3, step_size=0.0001)\n",
        "lambd = zfit.Parameter(\"lambda\", -3.0)\n",
        "sig_yield = zfit.Parameter('sig_yield', 5300, 0, 100000,\n",
        "                                step_size=10)  # step size: default is small, use appropriate\n",
        "bkg_yield = zfit.Parameter('bkg_yield', 25000, 0, 3e5, step_size=10)\n",
        "\n",
        "\n",
        "def getSWeights(centrality) -> np.array:\n",
        "    centralityCut=\"\"\n",
        "    if (centrality ==0):\n",
        "        centralityCut = \"(centrality==8)\"\n",
        "    elif (centrality ==1):\n",
        "        centralityCut = \"(centrality>=5) &( centrality<8) \"\n",
        "    elif (centrality ==2):\n",
        "        centralityCut = \"(centrality <5)\"\n",
        "    else:\n",
        "        print(\"Wrong centrality\")\n",
        "        return\n",
        "\n",
        "    with uproot.open(\"/home/prozorov/dev/star/D0_jets_2014_231030.root\") as exp_file:\n",
        "          exp_tree = exp_file['Jets']\n",
        "          exp = exp_tree.arrays([\"D0mass\"], cut=centralityCut + \" & (D0mass > 1.75) & (D0mass<2.0)\",library='pd')\n",
        "    \n",
        "# model building, pdf creation\n",
        "    signal_pdf  = zfit.pdf.Gauss(mu=mu, sigma=sigma, obs=obs)\n",
        "    comb_bkg_pdf  = zfit.pdf.Exponential(lambd, obs=obs)\n",
        "\n",
        "    data= zfit.Data.from_pandas(exp)\n",
        "\n",
        "# Create the extended models\n",
        "    extended_sig = signal_pdf.create_extended(sig_yield)\n",
        "    extended_bkg = comb_bkg_pdf.create_extended(bkg_yield)\n",
        "\n",
        "# The final model is the combination of the signal and backgrond PDF\n",
        "    model = zfit.pdf.SumPDF([extended_bkg, extended_sig])\n",
        "\n",
        "# plot the data\n",
        "    data_mass = data[\"D0mass\"].numpy()\n",
        "\n",
        "# Builds the loss.\n",
        "    data_sw = zfit.Data.from_numpy(obs=obs, array=data_mass)\n",
        "    nll_sw = zfit.loss.ExtendedUnbinnedNLL(model, data_sw)\n",
        "\n",
        "# This parameter was useful in the simultaneous fit but not anymore so we fix it.\n",
        "    sigma.floating = False\n",
        "\n",
        "# Minimizes the loss.\n",
        "    minimizer = zfit.minimize.Minuit(use_minuit_grad=True)\n",
        "    result_sw = minimizer.minimize(nll_sw)\n",
        "   \n",
        "    weights = compute_sweights(model, data_sw)\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 12. 15. 18. 30.]\n",
            "[0.86289331 0.89027367 0.91147481 0.92631992 0.93662837 0.94534112\n",
            " 0.95143901 0.95552521 0.95941312 0.96457069 0.97144899 0.97645541\n",
            " 0.97753223]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8902736681098666"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "with uproot.open(\"~/dev/star/unfold/JetFinderEfficiency.root\") as fileEff:\n",
        "    fileEff.classnames()\n",
        "    JetFinderEfficiency=fileEff[\"JetFinderEfficiency\"].to_numpy()\n",
        "    print(JetFinderEfficiency[1])\n",
        "    print(JetFinderEfficiency[0])\n",
        "\n",
        "def getEfficiencyPtWeight(pt):\n",
        "    ptBin = np.digitize(pt, JetFinderEfficiency[1])-1\n",
        "    if (ptBin<0):\n",
        "        ptBin=0\n",
        "    elif (ptBin>=len(JetFinderEfficiency[0])):\n",
        "        ptBin=len(JetFinderEfficiency[0])-1\n",
        "    return JetFinderEfficiency[0][ptBin]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define default plot styles  \n",
        "\n",
        "from matplotlib import rc\n",
        "import matplotlib.font_manager\n",
        "\n",
        "rc('font', family='serif')\n",
        "rc('text', usetex=False)\n",
        "rc('font', size=22)\n",
        "rc('xtick', labelsize=15)\n",
        "rc('ytick', labelsize=15)\n",
        "rc('legend', fontsize=15)\n",
        "\n",
        "plot_style_0 = {\n",
        "    'histtype': 'step',\n",
        "    'color': 'black',\n",
        "    'linewidth': 2,\n",
        "    'linestyle': '--',\n",
        "    'density': False\n",
        "}\n",
        "\n",
        "plot_style_1 = {\n",
        "    'histtype': 'step',\n",
        "    'color': 'black',\n",
        "    'linewidth': 2,\n",
        "    'density': False\n",
        "}\n",
        "\n",
        "plot_style_2 = {'alpha': 0.5, 'density': False}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IvPsjbMdcvYJ"
      },
      "outputs": [],
      "source": [
        "#@keras.saving.register_keras_serializable(package=\"CustomModel\", name=\"DNN2\")\n",
        "class DNN(keras.Model):\n",
        "    def __init__(self, sizes=(100, 100, 100), outputDims=2, inputDims=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._outputShape=outputDims\n",
        "        self._denseSizes=sizes\n",
        "        self._inputShape=inputShape\n",
        "\n",
        "        self._inputs = keras.Input(shape=inputDims)\n",
        "        self._layers = []\n",
        "        for i, size in enumerate(sizes):\n",
        "            _layer = layers.Dense(size, kernel_initializer=\"he_uniform\", kernel_regularizer=keras.regularizers.L1L2(l2=1e-4))\n",
        "            _activation = layers.Activation(\"relu\")\n",
        "            self._layers.extend([_layer, _activation])\n",
        "\n",
        "        _layer = layers.Dense(outputDims)\n",
        "        _activation = layers.Activation(\"softmax\")\n",
        "        self._layers.extend([_layer, _activation])\n",
        "\n",
        "        self._outputs = self.call(self._inputs)\n",
        "        self._model = keras.Model(self._inputs, self._outputs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self._tensors = [inputs]\n",
        "        for _layer in self._layers:\n",
        "            tensor = _layer(self._tensors[-1])\n",
        "            self._tensors.append(tensor)\n",
        "        return self._tensors[-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sizes\": self._denseSizes,\n",
        "            \"outputDims\": self._outputShape,\n",
        "            \"inputDims\": self._inputShape,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def model(self):\n",
        "        return self._model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Centrality',\n",
              " 'Weight',\n",
              " 'RefMult',\n",
              " 'gRefMult',\n",
              " 'RefCorr2',\n",
              " 'McRefMult',\n",
              " 'RecoRefMult',\n",
              " 'McD0Pt',\n",
              " 'McD0Eta',\n",
              " 'McD0Phi',\n",
              " 'McPionPt',\n",
              " 'McPionEta',\n",
              " 'McPionPhi',\n",
              " 'McKaonPt',\n",
              " 'McKaonEta',\n",
              " 'McKaonPhi',\n",
              " 'RecoD0Pt',\n",
              " 'RecoD0Eta',\n",
              " 'RecoD0Phi',\n",
              " 'RecoPionPt',\n",
              " 'RecoPionEta',\n",
              " 'RecoPionPhi',\n",
              " 'RecoKaonPt',\n",
              " 'RecoKaonEta',\n",
              " 'RecoKaonPhi',\n",
              " 'McJetPt',\n",
              " 'McJetEta',\n",
              " 'McJetPhi',\n",
              " 'McJetArea',\n",
              " 'McJetE',\n",
              " 'McJetNConst',\n",
              " 'McJetLambda_1_1',\n",
              " 'McJetLambda_1_1half',\n",
              " 'McJetLambda_1_2',\n",
              " 'McJetLambda_1_3',\n",
              " 'McJetD0Z',\n",
              " 'McRecoJetPt',\n",
              " 'McRecoJetEta',\n",
              " 'McRecoJetPhi',\n",
              " 'McRecoJetE',\n",
              " 'McRecoJetArea',\n",
              " 'McRecoJetNConst',\n",
              " 'McRecoJetLambda_1_1',\n",
              " 'McRecoJetLambda_1_1half',\n",
              " 'McRecoJetLambda_1_2',\n",
              " 'McRecoJetLambda_1_3',\n",
              " 'McRecoJetD0Z',\n",
              " 'RecoJetPt',\n",
              " 'RecoJetEta',\n",
              " 'RecoJetPhi',\n",
              " 'RecoJetArea',\n",
              " 'RecoJetE',\n",
              " 'RecoJetRhoVal',\n",
              " 'RecoJetNConst',\n",
              " 'RecoJetLambda_1_1',\n",
              " 'RecoJetLambda_1_1half',\n",
              " 'RecoJetLambda_1_2',\n",
              " 'RecoJetLambda_1_3',\n",
              " 'RecoJetD0Z']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trees = [uproot.open(\"/home/prozorov/dev/star/output_jets.root:Jets\"), ]\n",
        "\n",
        "trees[0].keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "centrality=trees[0][\"Centrality\"].array()\n",
        "plt.hist(centrality, bins=np.linspace(0, 80, 9), **plot_style_0)\n",
        "plt.xlabel(\"Centrality,%\")\n",
        "plt.ylabel(\"Counts\")\n",
        "\n",
        "centralityBins=[0,10,40,80]\n",
        "plt.hist(centrality, bins=centralityBins, **plot_style_2)\n",
        "plt.show()\n",
        "\n",
        "centralityCuts = [\"(Centrality > {}) & (Centrality < {})\".format(centralityBins[i], centralityBins[i+1]) for i in range(len(centralityBins)-1)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'uproot' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_1_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_1_1half\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_1_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_1_3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43muproot\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/prozorov/dev/star/output_jets.root\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m original_file:\n\u001b[1;32m      4\u001b[0m     original_tree \u001b[38;5;241m=\u001b[39m original_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m     sim \u001b[38;5;241m=\u001b[39m original_tree\u001b[38;5;241m.\u001b[39marrays([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecoJetPt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecoJetD0Z\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecoJetLambda_1_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecoJetLambda_1_1half\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecoJetLambda_1_2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecoJetLambda_1_3\u001b[39m\u001b[38;5;124m\"\u001b[39m], cut\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(RecoJetNConst>0) & (Centrality<10)\u001b[39m\u001b[38;5;124m\"\u001b[39m, library\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpd\u001b[39m\u001b[38;5;124m'\u001b[39m, entry_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'uproot' is not defined"
          ]
        }
      ],
      "source": [
        "columns = [\"pt\", \"z\", \"lambda_1_1\", \"lambda_1_1half\", \"lambda_1_2\", \"lambda_1_3\"]\n",
        "\n",
        "with uproot.open(\"/home/prozorov/dev/star/output_jets.root\") as original_file:\n",
        "    original_tree = original_file['Jets']\n",
        "    sim = original_tree.arrays([\"RecoJetPt\",\"RecoJetD0Z\",\"RecoJetLambda_1_1\",\n",
        "                            \"RecoJetLambda_1_1half\",\"RecoJetLambda_1_2\",\n",
        "                            \"RecoJetLambda_1_3\"], cut=\"(RecoJetNConst>0) & (Centrality<10)\", library='pd', entry_stop=1000000)\n",
        "    \n",
        "    sim.rename(columns={\"RecoJetPt\": \"pt\", \"RecoJetD0Z\": \"z\",  \"RecoJetLambda_1_1\": \"lambda_1_1\", \"RecoJetLambda_1_1half\": \"lambda_1_1half\", \"RecoJetLambda_1_2\": \"lambda_1_2\", \"RecoJetLambda_1_3\": \"lambda_1_3\"}, inplace=True)\n",
        "    \n",
        "with uproot.open(\"/home/prozorov/dev/star/D0_jets_2014_231030.root\") as target_file:\n",
        "    target_tree = target_file['Jets']\n",
        "    exp = target_tree.arrays([\"jet_pt_corr\",\"z\", \"lambda_1_1\",\"lambda_1_1half\",\"lambda_1_2\",\"lambda_1_3\"], cut=\"centrality==8\",library='pd')\n",
        "    exp.rename(columns={\"jet_pt_corr\": \"pt\"}, inplace=True)\n",
        "\n",
        "original_weights = np.ones(len(sim))\n",
        "\n",
        "# divide original samples into training ant test parts\n",
        "original_train, original_test = train_test_split(sim)\n",
        "# divide target samples into training ant test parts\n",
        "target_train, target_test = train_test_split(exp)\n",
        "\n",
        "original_weights_train = np.ones(len(original_train))\n",
        "original_weights_test = np.ones(len(original_test))\n",
        "\n",
        "from hep_ml.metrics_utils import ks_2samp_weighted\n",
        "\n",
        "hist_settings = {'bins': 50, 'density': True, 'alpha': 0.7}\n",
        "\n",
        "\n",
        "def draw_distributions(original, target, new_original_weights):\n",
        "    plt.figure(figsize=[15, 7])\n",
        "    for id, column in enumerate(columns, 1):\n",
        "        xlim = np.percentile(np.hstack([target[column]]), [1, 99])\n",
        "        plt.subplot(2, 3, id)\n",
        "        plt.hist(original[column], weights=new_original_weights, range=xlim, **hist_settings)\n",
        "        plt.hist(target[column], range=xlim, **hist_settings)\n",
        "        plt.yscale('log')\n",
        "        plt.title(column)\n",
        "        print('KS over ', column, ' = ', ks_2samp_weighted(original[column], target[column],\n",
        "                                         weights1=new_original_weights, weights2=np.ones(len(target), dtype=float)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "draw_distributions(original, target, original_weights)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "draw_distributions(original_train, target_train, original_weights_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "draw_distributions(original_test, target_test, original_weights_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# define base reweighter\n",
        "reweighter_base = reweight.GBReweighter(n_estimators=100, \n",
        "                                        learning_rate=0.1, max_depth=3, min_samples_leaf=1000, \n",
        "                                        gb_args={'subsample': 0.4})\n",
        "reweighter = reweight.FoldingReweighter(reweighter_base, n_folds=3)\n",
        "# it is not needed divide data into train/test parts; reweighter can be train on the whole samples\n",
        "reweighter.fit(original, target)\n",
        "\n",
        "folding_weights  = reweighter.predict_weights(original)\n",
        "# validate reweighting rule on the test part comparing 1d projections\n",
        "draw_distributions(original, target, folding_weights)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(gb_weights_test, bins=50)\n",
        "plt.yscale('log')\n",
        "plt.title('predicted weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxEyQZoLdHiD",
        "outputId": "d8d9de9d-1d90-4445-e705-e53224637ce2"
      },
      "outputs": [],
      "source": [
        "\n",
        "trees = [uproot.open(\"/home/prozorov/dev/star/output_jets.root:Jets\")]\n",
        "centralityBins=[0,80]\n",
        "centralityCuts = [\"(Centrality >= {}) & (Centrality < {})\".format(centralityBins[i], centralityBins[i+1]) for i in range(len(centralityBins)-1)]\n",
        "nEvents=100000\n",
        "\n",
        "features = [ \"pt\",\"z\" , \"lambda_1_1\", \"lambda_1_1half\",\"lambda_1_2\",\"lambda_1_3\"]\n",
        "\n",
        "# make centrality dependent analysis\n",
        "\n",
        "for i in range(len(centralityCuts)):\n",
        "  print(\"Processing centrality bin: \", centralityCuts[i])\n",
        "  \n",
        "  sim_mc_reco= trees[0].arrays([\"McJetPt\", \"McJetD0Z\", \n",
        "                               \"McJetLambda_1_1\", \"McJetLambda_1_1half\",\"McJetLambda_1_2\",\"McJetLambda_1_3\", \n",
        "                              \"RecoJetPt\",\"RecoJetD0Z\",\n",
        "                               \"RecoJetLambda_1_1\", \"RecoJetLambda_1_1half\",\"RecoJetLambda_1_2\",\"RecoJetLambda_1_3\",\"RecoJetNConst\"\n",
        "                            ],   \n",
        "                              cut=centralityCuts[i], library=\"pd\", entry_stop=nEvents)\n",
        "        \n",
        "  genJets = sim_mc_reco.iloc[:,0:6]\n",
        "  recoJets = sim_mc_reco.iloc[:,6:13]\n",
        "  genJets.rename(columns={\"McJetPt\": \"pt\", \"McJetD0Z\": \"z\",  \"McJetLambda_1_1\": \"lambda_1_1\", \"McJetLambda_1_1half\": \"lambda_1_1half\", \"McJetLambda_1_2\": \"lambda_1_2\", \"McJetLambda_1_3\": \"lambda_1_3\"}, inplace=True)\n",
        "  recoJets.rename(columns={\"RecoJetPt\": \"pt\", \"RecoJetD0Z\": \"z\" , \"RecoJetLambda_1_1\": \"lambda_1_1\", \"RecoJetLambda_1_1half\": \"lambda_1_1half\", \"RecoJetLambda_1_2\": \"lambda_1_2\", \"RecoJetLambda_1_3\": \"lambda_1_3\"}, inplace=True)\n",
        "\n",
        "  dummyval=-99\n",
        "  mask = recoJets[\"RecoJetNConst\"] == 0\n",
        "  recoJets.loc[mask,features[0]:features[-1]] = dummyval\n",
        "\n",
        "  print(recoJets.head(20))\n",
        "  print(genJets.head(20))\n",
        "\n",
        "  trainGen, testGen, trainReco, testReco = train_test_split(genJets, recoJets, test_size=0.2)\n",
        "\n",
        "  trainWts = np.ones(trainGen.shape[0])\n",
        "  print(\"Sum of training weights: \", trainWts.sum())\n",
        "\n",
        "  testWts = np.ones(testGen.shape[0])\n",
        "  print(\"Sum of testing weights: \", testWts.sum())\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  X_det = scaler.fit_transform(pd.concat([testReco[features], trainReco[features]], ignore_index=True, sort=False))\n",
        "  X_gen = scaler.fit_transform(pd.concat([trainGen[features], trainGen[features]], ignore_index=True, sort=False))\n",
        "\n",
        "  Y_det = utils.to_categorical(np.concatenate( (np.ones(testReco.shape[0]), np.zeros(trainReco.shape[0])) ))\n",
        "  Y_gen = utils.to_categorical(np.concatenate( (np.ones(trainGen.shape[0]), np.zeros(trainGen.shape[0])) ))\n",
        "\n",
        "  print(\"detector-level input shapes: \", X_det.shape, Y_det.shape)\n",
        "  print(\"generator-level input shapes: \", X_gen.shape, Y_gen.shape)\n",
        "\n",
        "\n",
        "  print(\"X_det.head(): \", X_det[0:5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFC2FnQbeafJ",
        "outputId": "147acade-bec8-4665-9004-dff2225f642e"
      },
      "outputs": [],
      "source": [
        "nData, nEmbedding = np.count_nonzero(Y_det[:,1]), np.count_nonzero(Y_det[:,0])\n",
        "#wtest[key] = np.ones(ntest[key])\n",
        "wData = testWts\n",
        "#wtrain[key] = ntest[key]/ntrain[key]*np.ones(ntrain[key])\n",
        "wEmbedding = (testWts.sum()/trainWts.sum()*trainWts)\n",
        "\n",
        "print(np.sum(wEmbedding), np.sum(wData))\n",
        "print(np.sum(testWts), np.sum(trainWts))\n",
        "\n",
        "folderPath = \"./weights/savedModel_\"\n",
        "\n",
        "lossFunc=\"binary_crossentropy\"\n",
        "optimizer=\"adam\"\n",
        "metricList=[\"accuracy\"]\n",
        "\n",
        "validationSize = 0.2\n",
        "nEpochs = 50\n",
        "batchSize = 1000\n",
        "\n",
        "inputShape = X_det.shape[1:]\n",
        "\n",
        "w_sim = [wEmbedding]\n",
        "nIter = 2\n",
        "earlystopping = EarlyStopping(patience=10,\n",
        "                              verbose=1,\n",
        "                              restore_best_weights=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# reweight the sim and data to have the same total weight to begin with\n",
        "nData, nEmbedding = np.count_nonzero(Y_det[:,1]), np.count_nonzero(Y_det[:,0])\n",
        "\n",
        "#wData = np.ones(nData)\n",
        "#wEmbedding = nData/float(nEmbedding)*np.ones(nEmbedding)\n",
        "\n",
        "wData = testWts.to_numpy()\n",
        "wEmbedding = (np.sum(wData)/trainWts.sum()*trainWts).to_numpy()\n",
        "\n",
        "print(np.sum(wEmbedding), np.sum(wData))\n",
        "print(np.sum(testWts), np.sum(trainWts))\n",
        "\n",
        "plt.hist(trainGen[\"pt\"], bins=30, weights=wEmbedding)\n",
        "plt.hist(trainReco[\"pt\"], bins=30, weights=wEmbedding)\n",
        "\n",
        "plt.yscale(\"log\")\n",
        "\n",
        "now = datetime.now()\n",
        "nownow = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "folderPath = \"savedModels/savedModel_\"+nownow\n",
        "unfoldingWeightsFilename = f\"{folderPath}/unfoldingWeights\"\n",
        "#logdir = \"logs/\"+nownow\n",
        "\n",
        "lossFunc=\"categorical_crossentropy\"\n",
        "optimizer=\"adam\"\n",
        "metricList=[\"accuracy\"]\n",
        "#weightedMetricList = [\"categorical_crossentropy\"]\n",
        "weightedMetricList = []\n",
        "\n",
        "patience=5\n",
        "\n",
        "validationSize = 0.2\n",
        "nEpochs = 150\n",
        "batchSize = 2048\n",
        "\n",
        "inputShape = X_det.shape[1:]\n",
        "\n",
        "det_history = []\n",
        "gen_history = []\n",
        "w_sim = [wEmbedding]\n",
        "nIter = 10\n",
        "\n",
        "weightClipMin = 0.\n",
        "weightClipMax = np.inf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(nIter):\n",
        "    detModel = sequentialDNNMaker(input_shape=inputShape)\n",
        "    detModel.compile(loss=lossFunc, optimizer=optimizer, metrics=metricList, weighted_metrics=weightedMetricList)\n",
        "    detCallBacks = [callbacks.EarlyStopping(patience=patience, verbose=1, restore_best_weights=True)]\n",
        "    detModelFilePath = folderPath + f\"/step1_iteration{i}\"+\"_epoch{epoch}\"\n",
        "    detCallBacks.append(callbacks.ModelCheckpoint(detModelFilePath, save_best_only=True, verbose=1))\n",
        "    #detCallBacks.append(callbacks.TensorBoard(log_dir=logdir, histogram_freq=1))\n",
        "\n",
        "    genModel = sequentialDNNMaker(input_shape=inputShape)\n",
        "    genModel.compile(loss=lossFunc, optimizer=optimizer, metrics=metricList, weighted_metrics=weightedMetricList)\n",
        "    genCallBacks = [callbacks.EarlyStopping(patience=patience, verbose=1, restore_best_weights=True)]\n",
        "    genModelFilePath = folderPath + f\"/step2_iteration{i}\"+\"_epoch{epoch}\"\n",
        "    genCallBacks.append(callbacks.ModelCheckpoint(genModelFilePath, save_best_only=True, verbose=1))\n",
        "    #genCallBacks.append(callbacks.TensorBoard(log_dir=logdir, histogram_freq=1))\n",
        "\n",
        "    if(i > 0):\n",
        "        detModel.load_weights(folderPath + f\"/step1_iteration{i-1}\")\n",
        "        genModel.load_weights(folderPath + f\"/step2_iteration{i-1}\")\n",
        "\n",
        "    w_det = np.concatenate([wData, w_sim[-1]])\n",
        "    \n",
        "    X_det_train, X_det_val, Y_det_train, Y_det_val, w_det_train, w_det_val = train_test_split(X_det, Y_det, w_det, test_size=validationSize)\n",
        "    detModel.summary()\n",
        "    det_history.append(detModel.fit(X_det_train, Y_det_train, sample_weight=w_det_train, epochs=nEpochs, batch_size=batchSize, validation_data=(X_det_val, Y_det_val, w_det_val), verbose=1, callbacks=detCallBacks))\n",
        "    detModel.save_weights(folderPath + f\"/step1_iteration{i}\")\n",
        "    \n",
        "    prediction = detModel.predict(X_det, batch_size=batchSize*10)\n",
        "    scaleFactors = prediction[Y_det[:, 0] == 1]\n",
        "\n",
        "    _pull = np.clip(scaleFactors[:, 1]/(scaleFactors[:, 0]+ 10**-50), weightClipMin, weightClipMax)*w_sim[-1]\n",
        "    w_sim.append(_pull)\n",
        "\n",
        "    w_gen = np.concatenate([w_sim[-1], w_sim[-2]])\n",
        "    \n",
        "    X_gen_train, X_gen_val, Y_gen_train, Y_gen_val, w_gen_train, w_gen_val = train_test_split(X_gen, Y_gen, w_gen, test_size=validationSize)\n",
        "    genModel.summary()\n",
        "    gen_history.append(genModel.fit(X_gen_train, Y_gen_train, sample_weight=w_gen_train, epochs=nEpochs, batch_size=5*batchSize, validation_data=(X_gen_val, Y_gen_val, w_gen_val), verbose=1, callbacks=genCallBacks))\n",
        "    genModel.save_weights(folderPath + f\"/step2_iteration{i}\")\n",
        "    \n",
        "    prediction = genModel.predict(X_gen, batch_size=batchSize*50)\n",
        "    scaleFactors = prediction[Y_gen[:, 0] == 1]\n",
        "\n",
        "    _push = np.clip(scaleFactors[:, 1]/(scaleFactors[:, 0]+ 10**-50), weightClipMin, weightClipMax)*w_sim[-1]\n",
        "    w_sim.append(_push)\n",
        "    \n",
        "    np.save(unfoldingWeightsFilename, w_sim)\n",
        "\n",
        "#filename = f\"outputs/multifoldClosure_{patience}_{batchSize}_{nEpochs}_{nIter}_{nownow}.root\"\n",
        "#outFile = uproot.recreate(filename)\n",
        "#print(\"Saving to file: \", filename)\n",
        "\n",
        "#trainGen[\"wt\"] = w_sim[2*nIter]\n",
        "#outFile[\"unfolded\"] = trainGen\n",
        "#outFile[\"reco\"] = testReco\n",
        "#outFile[\"gen\"] = testGen\n",
        "wu=w_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "plt.rcParams['figure.dpi'] = 240\n",
        "plt.rcParams.update({\n",
        "    \"text.usetex\": True,\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.sans-serif\": \"Helvetica\",\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a dictionary to hold information about the observables\n",
        "obs = {}\n",
        "# the jet mass and histogram style information\n",
        "obs.setdefault('pt', {}).update({\n",
        "    'func': lambda dset, type: dset[type],\n",
        "    'nbins_det': 20, 'nbins_mc': 50,\n",
        "    'xlim': (-1, 30),  'ylim': (0.00000001, 0.3),\n",
        "    'xlabel': r'Jet $p_{t}$  [GeV/c]', 'symbol': r'$p_{t}$',\n",
        "    'ylabel': r'Counts', 'yscale': 'log',\n",
        "    'stamp_xy': (0.425, 0.65),\n",
        "})\n",
        "\n",
        "# the groomed momentum fraction and histogram style information\n",
        "obs.setdefault('z', {}).update({\n",
        "    'func': lambda dset, type: dset[type],\n",
        "    'nbins_det': 20, 'nbins_mc': 50,\n",
        "    'xlim': (0.1, 1.001),   'ylim': (0, 10),\n",
        "    'xlabel': r'Jet Momentum Fraction $z$', 'symbol': r'$z$',\n",
        "    'ylabel': r'Counts',\n",
        "    'stamp_xy': (0.425, 0.65),\n",
        "})\n",
        "\n",
        "obs.setdefault('lambda_1_1', {}).update({\n",
        "    'func': lambda dset, type: dset[type],\n",
        "    'nbins_det': 20, 'nbins_mc': 50,\n",
        "    'xlim': (0, 0.8),  'ylim': (0, 10),\n",
        "    'xlabel': r'Jet Angularity $\\lambda_{1}^{1}$', 'symbol': r'$lambda_1_1$',\n",
        "    'ylabel': r'Counts',\n",
        "    'stamp_xy': (0.425, 0.65),\n",
        "})\n",
        "\n",
        "obs.setdefault('lambda_1_1half', {}).update({\n",
        "    'func': lambda dset, type: dset[type],\n",
        "    'nbins_det': 20, 'nbins_mc': 50,\n",
        "    'xlim': (0, 0.4),  'ylim': (0, 10),\n",
        "    'xlabel': r'Jet Angularity $\\lambda_{1}^{1/2}$', 'symbol': r'$lambda_1_1half$',\n",
        "    'ylabel': r'Counts',\n",
        "    'stamp_xy': (0.425, 0.65),\n",
        "})\n",
        "\n",
        "obs.setdefault('lambda_1_2', {}).update({\n",
        "    'func': lambda dset, type: dset[type],\n",
        "    'nbins_det': 20, 'nbins_mc': 50,\n",
        "    'xlim': (0, 0.2),  'ylim': (0, 10),\n",
        "    'xlabel': r'Jet Angularity $\\lambda_{1}^{2}$', 'symbol': r'$lambda_1_2$',\n",
        "    'ylabel': r'Counts',\n",
        "    'stamp_xy': (0.425, 0.65),\n",
        "})\n",
        "\n",
        "obs.setdefault('lambda_1_3', {}).update({\n",
        "    'func': lambda dset, type: dset[type],\n",
        "    'nbins_det': 20, 'nbins_mc': 50,\n",
        "    'xlim': (0, 0.1),  'ylim': (0, 10),\n",
        "    'xlabel': r'Jet Angularity $\\lambda_{1}^{3}$', 'symbol': r'$lambda_1_3$',\n",
        "    'ylabel': r'Counts',\n",
        "    'stamp_xy': (0.425, 0.65),\n",
        "})\n",
        "\n",
        "\n",
        "# additional histogram and plot style information\n",
        "hist_style = {'histtype': 'step', 'density': True, 'lw': 1, 'zorder': 2}\n",
        "gen_style = {'linestyle': '--', 'color': 'blue', 'lw': 1.15, 'label': 'Gen.'}\n",
        "truth_style = {'step': 'mid', 'edgecolor': 'green', 'facecolor': (0.75, 0.875, 0.75),\n",
        "               'lw': 1.25, 'zorder': 0, 'label': '``Truth\\\"'}\n",
        "omnifold_style = {'ls': '-', 'marker': 's', 'ms': 2.5, 'color': 'tab:red', 'zorder': 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Unfolding Results\n",
        "\n",
        "Now it's time to plot the unfolding results for all of the specified observables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for obkey,ob in obs.items():\n",
        "        # calculate observable for GEN, SIM, DATA\n",
        "    ob['genobs'] = trainGen[obkey].to_numpy()\n",
        "    ob['simobs'] = trainReco[obkey].to_numpy()\n",
        "\n",
        "    ob['truthobs'] = testGen[obkey].to_numpy()\n",
        "    ob['dataobs'] = testReco[obkey].to_numpy()\n",
        "\n",
        " # setup bins\n",
        "    ob['bins_det'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_det']+1)\n",
        "    ob['bins_mc'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_mc']+1)\n",
        "    ob['midbins_det'] = (ob['bins_det'][:-1] + ob['bins_det'][1:])/2\n",
        "    ob['midbins_mc'] = (ob['bins_mc'][:-1] + ob['bins_mc'][1:])/2\n",
        "    ob['binwidth_det'] = ob['bins_det'][1] - ob['bins_det'][0]\n",
        "    ob['binwidth_mc'] = ob['bins_mc'][1] - ob['bins_mc'][0]\n",
        "\n",
        "\n",
        "\n",
        "    # get the histograms of GEN, DATA, and SIM level observables\n",
        "    ob['genobs_hist'] = np.histogram(ob['genobs'], bins=ob['bins_mc'])[0]\n",
        "    ob['simobs_hist'] = np.histogram(ob['simobs'], bins=ob['bins_det'])[0]\n",
        "    ob['data_hist']   = np.histogram(ob['dataobs'], bins=ob['bins_det'])[0]\n",
        "  \n",
        "    ob['truth_hist'], ob['truth_hist_unc'] = modplot.calc_hist(ob['truthobs'], bins=ob['bins_mc'], \n",
        "                                                               density=True)[:2]\n",
        "  \n",
        "   \n",
        "    print('Done with', obkey)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i,(obkey,ob) in enumerate(obs.items()):\n",
        "    \n",
        "    # get the styled axes on which to plot\n",
        "    fig, [ax0, ax1] = modplot.axes(**ob)\n",
        "    if ob.get('yscale') is not None:\n",
        "        ax0.set_yscale(ob['yscale'])\n",
        "\n",
        "\n",
        "    # Plot the Different Distributions of the Observable\n",
        "    # plot the \"data\" histogram of the observable\n",
        "    ax0.hist(ob['dataobs'], bins=ob['bins_det'], color='black', label='``Data\\\"', **hist_style)\n",
        "\n",
        "    # plot the \"sim\" histogram of the observable\n",
        "    ax0.hist(ob['simobs'], bins=ob['bins_det'], color='orange', label='Sim.', **hist_style)\n",
        "\n",
        "\n",
        "    ax0.hist(ob['genobs'], bins=ob['bins_mc'], color='blue', label='``Gen\\\"', **hist_style)\n",
        "\n",
        "    # plot the \"sim\" histogram of the observable\n",
        "    ax0.hist(ob['truthobs'], bins=ob['bins_mc'], color='green', label='Truth.', **hist_style)\n",
        "\n",
        "\n",
        "    # # plot the \"gen\" histogram of the observable\n",
        "    # ax0.plot(ob['midbins_mc'], ob['genobs_hist'], **gen_style)\n",
        "\n",
        "    # plot the \"truth\" histogram of the observable\n",
        "    ax0.fill_between(ob['midbins_mc'], ob['truth_hist'], **truth_style)\n",
        "\n",
        "    \n",
        "    # Plot the Unfolded Distributions of the Observable\n",
        "    # plot the OmniFold distribution\n",
        "    of_histgen, of_histgen_unc = modplot.calc_hist(ob['genobs'], weights=w_sim[-1],\n",
        "                                                   bins=ob['bins_mc'], density=True)[:2]\n",
        "    ax0.plot(ob['midbins_mc'], of_histgen, **omnifold_style, label='MultiFold')\n",
        "\n",
        "    # Plot the Ratios of the OmniFold distribution to truth (with statistical uncertainties)\n",
        "\n",
        "    of_ratio = of_histgen/(ob['truth_hist']+ 10**-50)\n",
        "    ax1.plot(ob['midbins_mc'], of_ratio, **omnifold_style)\n",
        "\n",
        "    ax1.plot([np.min(ob['midbins_mc']), np.max(ob['midbins_mc'])], [1, 1], '-', color='green', lw=0.75)\n",
        "    \n",
        "    # ratio uncertainties\n",
        "    truth_unc_ratio = ob['truth_hist_unc']/(ob['truth_hist'] + 10**-50)\n",
        "\n",
        "    of_unc_ratio = of_histgen_unc/(ob['truth_hist'] + 10**-50)\n",
        "    \n",
        "    ax1.fill_between(ob['midbins_mc'], 1 - truth_unc_ratio, 1 + truth_unc_ratio, \n",
        "                     facecolor=truth_style['facecolor'], zorder=-2)\n",
        "\n",
        "    ax1.errorbar(ob['midbins_mc'], of_ratio, xerr=ob['binwidth_mc']/2, yerr=of_unc_ratio, \n",
        "                                              color=omnifold_style['color'], **modplot.style('errorbar'))\n",
        "\n",
        "    # legend style and ordering\n",
        "    loc, ncol = ob.get('legend_loc', 'upper right'), ob.get('legend_ncol', 2)\n",
        "    order = [3, 2, 0, 1] if ncol==2 else [3, 0, 2, 1]\n",
        "    modplot.legend(ax=ax0, frameon=False, loc=loc, ncol=ncol)\n",
        "\n",
        "    # stamp to put on the plots\n",
        "    modplot.stamp(*ob['stamp_xy'], delta_y=0.06, ax=ax0,\n",
        "                  line_0=r'\\textbf{test}'\n",
        ")\n",
        "\n",
        "\n",
        "    # save plot (by default in the same directory as this notebook).\n",
        "    # If running on binder, the plot can be accessed by first going to the jupyter file browser\n",
        "    # (which itself can be accessed by copying the URL of this notebook and removing the name of the notebook\n",
        "    # after the final \"/\"), selecting the square next to the name of the plot, and clicking \"Download\".\n",
        "    fig.savefig('MultiFold_{}.pdf'.format(obkey), bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_iter = 6\n",
        "nRows = 3\n",
        "nColumns = 3\n",
        "fig, ax = plt.subplots(nRows, nColumns, figsize=(30., 30.))\n",
        "for i, feature in enumerate(featuresNoWt):\n",
        "    row = int(i/nColumns)\n",
        "    column = i%nColumns \n",
        "        \n",
        "    histGen, bins = np.histogram(testGen[feature], weights=wData, density=True)\n",
        "    histReco, bins = np.histogram(testReco[feature], weights=wData, density=True) \n",
        "    histUnfolded, bins = np.histogram(trainGen[feature], weights=wu[2*_iter], density=True)\n",
        "        \n",
        "    ax[row, column].hist(bins[:-1], bins, weights=histReco/histGen, label=\"Reco/Gen\", histtype=\"step\")\n",
        "    ax[row, column].hist(bins[:-1], bins, weights=histUnfolded/histGen, label=\"Unfolded/Gen\", histtype=\"step\")\n",
        "    ax[row, column].hist(bins[:-1], bins, weights=np.ones((bins.shape[0]-1)), histtype=\"step\")\n",
        "    ax[row, column].set_title(feature)\n",
        "    ax[row, column].set_ylim(0.5, 1.5)\n",
        "    ax[row, column].legend(prop={'size': 15})\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_nIter = 10\n",
        "\n",
        "#metrics = [\"loss\", \"accuracy\", \"categorical_crossentropy\"]\n",
        "metrics = [\"loss\", \"accuracy\"]\n",
        "detfig, detax = plt.subplots(len(metrics), _nIter, figsize=(_nIter*5., len(metrics)*5.))\n",
        "for row, metricName in enumerate(metrics):\n",
        "    for column in range(_nIter):\n",
        "        detax[row, column].plot(det_history[column].history[metricName], label=\"training\")\n",
        "        detax[row, column].plot(det_history[column].history[\"val_\"+metricName], label=\"validation\")\n",
        "        detax[row, column].set_title(metricName+f\", detector level, iteration={column+1}\")\n",
        "        #detax[row, column].set_ylim(0.25, 0.32)\n",
        "        detax[row, column].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "genfig, genax = plt.subplots(len(metrics), _nIter, figsize=(_nIter*5., len(metrics)*5.))\n",
        "for row, metricName in enumerate(metrics):\n",
        "    for column in range(_nIter):\n",
        "        genax[row, column].plot(gen_history[column].history[metricName], label=\"training\")\n",
        "        genax[row, column].plot(gen_history[column].history[\"val_\"+metricName], label=\"validation\")\n",
        "        genax[row, column].set_title(metricName+f\", gen level, iteration={column+1}\")\n",
        "        genax[row, column].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(det_history[0].history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
