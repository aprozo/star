{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmniFold Demo\n",
    "\n",
    "In this tutorial, we showcase the OmniFold method for universally unfolding collider data using synthetic datasets of jets.\n",
    "\n",
    "Unfolding is the problem of estimating the particle-level (`truth`) information from the measured detector-level data (`data`). The detector is imperfect and smears the particle-level radiation pattern, giving rise to the need to unfold, which is particularly important for jet substructure. OmniFold uses a synthetic dataset where the particle-level (`generation`) and detector-level (`simulation`) information are both known.\n",
    "\n",
    "OmniFold is an iterative unfolding procedure consisting of two steps.\n",
    "* First, the `simulation` is reweighted to the `data`.\n",
    "* Second, the previous `generation` is reweighted to the new `generation`.\n",
    "\n",
    "OmniFold results in a set of weights for the `generation` that reweight it to an estimate for the `truth` distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 23:09:05.747956: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-20 23:09:06.106556: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-20 23:09:07.422074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/prozorov/install/miniconda3/envs/tf/lib/python3.11/site-packages/energyflow/archs/__init__.py:30: UserWarning: could not import some architectures - cannot import name '__version__' from 'tensorflow.keras' (/home/prozorov/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/api/_v2/keras/__init__.py)\n",
      "  warnings.warn('could not import some architectures - ' + str(e))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import energyflow as ef\n",
    "import energyflow.archs\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import omnifold\n",
    "import modplot\n",
    "import ibu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,4)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Datasets\n",
    "\n",
    "The datasets consists of jets that are produced in association with a $Z$ boson ($Z$+jet with $p_T^Z>200$ GeV).\n",
    "Jets are clustered with the anti-$k_T$ algorithm with a radius of $R=0.4$ and the leading (highest $p_T$) jet is taken. This procedure is done with several tunes of Pythia and the default tune of Hergwig, both before and after passing through a DELPHES simulation of the CMS detector.\n",
    "\n",
    "These samples are publicly available on [Zenodo](https://zenodo.org/record/3548091#.XdbOvjJKhlo), with automatic download and read-in provided by the [EnergyFlow](https://energyflow.network) Python package.\n",
    "\n",
    "**Customize**: Read in `'Pythia21'` and `'Pythia25'` to explore other tunes. Change `num_data` to vary the amount of data, `-1` is the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'Pythia26': ef.zjets_delphes.load('Pythia26', num_data=100000, exclude_keys=['particles']),\n",
    "            'Herwig': ef.zjets_delphes.load('Herwig', num_data=100000, exclude_keys=['particles'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Unfolding Problem\n",
    "\n",
    "OmniFold requires particle-level event generators (here, Pythia or Herwig) as well as a faithful simulation of the detector (here, DELPHES). By default we will treat Herwig as the natural \"truth\"/\"data\" and Pythia as the synthetic \"generation\"/\"simulation\".\n",
    "\n",
    "**Customize**: Change which tune of Pythia is used and which generator is used as `synthetic` or `nature`. Change plot stamps below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose what is MC and Data in this context\n",
    "synthetic, nature = datasets['Pythia26'], datasets['Herwig']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to specify `itnum`: how many iterations of the unfolding procedure we want to do.\n",
    "\n",
    "**Customize**: Change `itnum` to your desired number of unfolding iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many iterations of the unfolding process\n",
    "itnum = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three flavors of OmniFold. In order of increasing sophistication, they are:\n",
    "* **UniFold**: Represent the jet as a single observable.\n",
    "* **MultiFold**: Represent the jet as multiple observables.\n",
    "* **OmniFold**: Represent the jet as a set of particles.\n",
    "\n",
    "By default, we will implement MultiFold and represent the jet using six jet substructure observbles:\n",
    "* `'Mass'`, Jet Mass $m$: the invaraiant mass of the jet four-vector\n",
    "* `'Mult'`, Constituent Multiplicity $M$: the number of particles in the jet\n",
    "* `'Width'`, Jet Width $w$: a measure of how broad the jet is\n",
    "* `'Tau21'`, $N$-subjettiness Ratio $\\tau_{21}$: a measure of how two-pronged the jet is\n",
    "* `'zg'`, Groomed Momentum Fraction $z_g$: the energy-sharing of the prongs after grooming\n",
    "* `'SDMass'`, Groomed Jet Mass $m_{\\rm SD}$: the invariant mass of the jet four-vector after grooming\n",
    "\n",
    "**Customize**: Change which observables are used in MultiFold. UniFold corresponds to using a single observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_multifold = ['Mass', 'Mult', 'Width', 'Tau21', 'zg', 'SDMass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observables are already computed in the samples. We will read them in as an observable dictionary `obs` and also specify histogram style information.\n",
    "\n",
    "**Customize**: Add entries to `obs` to define your own observables to be used in MultiFold or to see the unfolding performance on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to hold information about the observables\n",
    "obs = {}\n",
    "\n",
    "# the jet mass and histogram style information\n",
    "obs.setdefault('Mass', {}).update({\n",
    "    'func': lambda dset, ptype: dset[ptype + '_jets'][:,3],\n",
    "    'nbins_det': 50, 'nbins_mc': 50,\n",
    "    'xlim': (0, 75), 'ylim': (0, 0.065),\n",
    "    'xlabel': r'Jet Mass $m$ [GeV]', 'symbol': r'$m$',\n",
    "    'ylabel': r'Normalized Cross Section [GeV$^{-1}$]',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the constituent multiplicity and histogram style information\n",
    "obs.setdefault('Mult', {}).update({\n",
    "    'func': lambda dset, ptype: dset[ptype + '_mults'],\n",
    "    'nbins_det': 80, 'nbins_mc': 80,\n",
    "    'xlim': (0, 80), 'ylim': (0, 0.065),\n",
    "    'xlabel': 'Jet Constituent Multiplicity $M$', 'symbol': r'$M$',\n",
    "    'ylabel': r'Normalized Cross Section',\n",
    "    'stamp_xy': (0.42, 0.65),\n",
    "})\n",
    "\n",
    "# the jet width and histogram style information\n",
    "obs.setdefault('Width', {}).update({\n",
    "    'func': lambda dset, ptype: dset[ptype + '_widths'],\n",
    "    'nbins_det': 50, 'nbins_mc': 50,\n",
    "    'xlim': (0, 0.6), 'ylim': (0, 10),\n",
    "    'xlabel': r'Jet Width $w$', 'symbol': r'$w$',\n",
    "    'ylabel': r'Normalized Cross Section',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the N-subjettiness ratio and histogram style information\n",
    "obs.setdefault('Tau21', {}).update({\n",
    "    'func': lambda dset, ptype: dset[ptype + '_tau2s']/(dset[ptype + '_widths'] + 10**-50),\n",
    "    'nbins_det': 50, 'nbins_mc': 50,\n",
    "    'xlim': (0, 1.2), 'ylim': (0, 3),\n",
    "    'xlabel': r'$N$-subjettiness Ratio $\\tau_{21}^{(\\beta=1)}$', 'symbol': r'$\\tau_{21}^{(\\beta=1)}$',\n",
    "    'ylabel': r'Normalized Cross Section',\n",
    "    'stamp_xy': (0.41, 0.92),\n",
    "    'legend_loc': 'upper left', 'legend_ncol': 1,\n",
    "})\n",
    "\n",
    "# the groomed momentum fraction and histogram style information\n",
    "obs.setdefault('zg', {}).update({\n",
    "    'func': lambda dset, ptype: dset[ptype + '_zgs'],\n",
    "    'nbins_det': 50, 'nbins_mc': 50,\n",
    "    'xlim': (0, 0.5), 'ylim': (0, 9),\n",
    "    'xlabel': r'Groomed Jet Momentum Fraction $z_g$', 'symbol': r'$z_g$',\n",
    "    'ylabel': 'Normalized Cross Section',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the groomed jet mass and histogram style information\n",
    "obs.setdefault('SDMass', {}).update({\n",
    "    'func': lambda dset, ptype: np.log(dset[ptype + '_sdms']**2/\n",
    "                                       dset[ptype + '_jets'][:,0]**2 + 10**-100),\n",
    "    'nbins_det': 50, 'nbins_mc': 50,\n",
    "    'xlim': (-14, -2), 'ylim': (0, 0.3),\n",
    "    'xlabel': r'Soft Drop Jet Mass $\\ln\\rho$', 'symbol': r'$\\ln\\rho$',\n",
    "    'ylabel': r'Normalized Cross Section',\n",
    "    'stamp_xy': (0.41, 0.92),\n",
    "    'legend_loc': 'upper left', 'legend_ncol': 1,\n",
    "})\n",
    "\n",
    "# additional histogram and plot style information\n",
    "hist_style = {'histtype': 'step', 'density': True, 'lw': 1, 'zorder': 2}\n",
    "gen_style = {'linestyle': '--', 'color': 'blue', 'lw': 1.15, 'label': 'Gen.'}\n",
    "truth_style = {'step': 'mid', 'edgecolor': 'green', 'facecolor': (0.75, 0.875, 0.75),\n",
    "               'lw': 1.25, 'zorder': 0, 'label': '``Truth\\\"'}\n",
    "ibu_style = {'ls': '-', 'marker': 'o', 'ms': 2.5, 'color': 'gray', 'zorder': 1}\n",
    "omnifold_style = {'ls': '-', 'marker': 's', 'ms': 2.5, 'color': 'tab:red', 'zorder': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that remains is to get the values of the specified observables and compute the histograms with the specified binnings. As an unfolding benchmark, we also obtain the unfolding results of Iterative Bayesian Unfolding (IBU) as implemented in `ibu.py`. The following cell takes care of all of these aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75  2.25  3.75  5.25  6.75  8.25  9.75 11.25 12.75 14.25 15.75 17.25\n",
      " 18.75 20.25 21.75 23.25 24.75 26.25 27.75 29.25 30.75 32.25 33.75 35.25\n",
      " 36.75 38.25 39.75 41.25 42.75 44.25 45.75 47.25 48.75 50.25 51.75 53.25\n",
      " 54.75 56.25 57.75 59.25 60.75 62.25 63.75 65.25 66.75 68.25 69.75 71.25\n",
      " 72.75 74.25]\n",
      "[ 0.75  2.25  3.75  5.25  6.75  8.25  9.75 11.25 12.75 14.25 15.75 17.25\n",
      " 18.75 20.25 21.75 23.25 24.75 26.25 27.75 29.25 30.75 32.25 33.75 35.25\n",
      " 36.75 38.25 39.75 41.25 42.75 44.25 45.75 47.25 48.75 50.25 51.75 53.25\n",
      " 54.75 56.25 57.75 59.25 60.75 62.25 63.75 65.25 66.75 68.25 69.75 71.25\n",
      " 72.75 74.25]\n",
      "Done with Mass\n",
      "[ 0.5  1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5 11.5 12.5 13.5\n",
      " 14.5 15.5 16.5 17.5 18.5 19.5 20.5 21.5 22.5 23.5 24.5 25.5 26.5 27.5\n",
      " 28.5 29.5 30.5 31.5 32.5 33.5 34.5 35.5 36.5 37.5 38.5 39.5 40.5 41.5\n",
      " 42.5 43.5 44.5 45.5 46.5 47.5 48.5 49.5 50.5 51.5 52.5 53.5 54.5 55.5\n",
      " 56.5 57.5 58.5 59.5 60.5 61.5 62.5 63.5 64.5 65.5 66.5 67.5 68.5 69.5\n",
      " 70.5 71.5 72.5 73.5 74.5 75.5 76.5 77.5 78.5 79.5]\n",
      "[ 0.5  1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5 11.5 12.5 13.5\n",
      " 14.5 15.5 16.5 17.5 18.5 19.5 20.5 21.5 22.5 23.5 24.5 25.5 26.5 27.5\n",
      " 28.5 29.5 30.5 31.5 32.5 33.5 34.5 35.5 36.5 37.5 38.5 39.5 40.5 41.5\n",
      " 42.5 43.5 44.5 45.5 46.5 47.5 48.5 49.5 50.5 51.5 52.5 53.5 54.5 55.5\n",
      " 56.5 57.5 58.5 59.5 60.5 61.5 62.5 63.5 64.5 65.5 66.5 67.5 68.5 69.5\n",
      " 70.5 71.5 72.5 73.5 74.5 75.5 76.5 77.5 78.5 79.5]\n",
      "Done with Mult\n",
      "[0.006 0.018 0.03  0.042 0.054 0.066 0.078 0.09  0.102 0.114 0.126 0.138\n",
      " 0.15  0.162 0.174 0.186 0.198 0.21  0.222 0.234 0.246 0.258 0.27  0.282\n",
      " 0.294 0.306 0.318 0.33  0.342 0.354 0.366 0.378 0.39  0.402 0.414 0.426\n",
      " 0.438 0.45  0.462 0.474 0.486 0.498 0.51  0.522 0.534 0.546 0.558 0.57\n",
      " 0.582 0.594]\n",
      "[0.006 0.018 0.03  0.042 0.054 0.066 0.078 0.09  0.102 0.114 0.126 0.138\n",
      " 0.15  0.162 0.174 0.186 0.198 0.21  0.222 0.234 0.246 0.258 0.27  0.282\n",
      " 0.294 0.306 0.318 0.33  0.342 0.354 0.366 0.378 0.39  0.402 0.414 0.426\n",
      " 0.438 0.45  0.462 0.474 0.486 0.498 0.51  0.522 0.534 0.546 0.558 0.57\n",
      " 0.582 0.594]\n",
      "Done with Width\n",
      "[0.012 0.036 0.06  0.084 0.108 0.132 0.156 0.18  0.204 0.228 0.252 0.276\n",
      " 0.3   0.324 0.348 0.372 0.396 0.42  0.444 0.468 0.492 0.516 0.54  0.564\n",
      " 0.588 0.612 0.636 0.66  0.684 0.708 0.732 0.756 0.78  0.804 0.828 0.852\n",
      " 0.876 0.9   0.924 0.948 0.972 0.996 1.02  1.044 1.068 1.092 1.116 1.14\n",
      " 1.164 1.188]\n",
      "[0.012 0.036 0.06  0.084 0.108 0.132 0.156 0.18  0.204 0.228 0.252 0.276\n",
      " 0.3   0.324 0.348 0.372 0.396 0.42  0.444 0.468 0.492 0.516 0.54  0.564\n",
      " 0.588 0.612 0.636 0.66  0.684 0.708 0.732 0.756 0.78  0.804 0.828 0.852\n",
      " 0.876 0.9   0.924 0.948 0.972 0.996 1.02  1.044 1.068 1.092 1.116 1.14\n",
      " 1.164 1.188]\n",
      "Done with Tau21\n",
      "[0.005 0.015 0.025 0.035 0.045 0.055 0.065 0.075 0.085 0.095 0.105 0.115\n",
      " 0.125 0.135 0.145 0.155 0.165 0.175 0.185 0.195 0.205 0.215 0.225 0.235\n",
      " 0.245 0.255 0.265 0.275 0.285 0.295 0.305 0.315 0.325 0.335 0.345 0.355\n",
      " 0.365 0.375 0.385 0.395 0.405 0.415 0.425 0.435 0.445 0.455 0.465 0.475\n",
      " 0.485 0.495]\n",
      "[0.005 0.015 0.025 0.035 0.045 0.055 0.065 0.075 0.085 0.095 0.105 0.115\n",
      " 0.125 0.135 0.145 0.155 0.165 0.175 0.185 0.195 0.205 0.215 0.225 0.235\n",
      " 0.245 0.255 0.265 0.275 0.285 0.295 0.305 0.315 0.325 0.335 0.345 0.355\n",
      " 0.365 0.375 0.385 0.395 0.405 0.415 0.425 0.435 0.445 0.455 0.465 0.475\n",
      " 0.485 0.495]\n",
      "Done with zg\n",
      "[-13.88 -13.64 -13.4  -13.16 -12.92 -12.68 -12.44 -12.2  -11.96 -11.72\n",
      " -11.48 -11.24 -11.   -10.76 -10.52 -10.28 -10.04  -9.8   -9.56  -9.32\n",
      "  -9.08  -8.84  -8.6   -8.36  -8.12  -7.88  -7.64  -7.4   -7.16  -6.92\n",
      "  -6.68  -6.44  -6.2   -5.96  -5.72  -5.48  -5.24  -5.    -4.76  -4.52\n",
      "  -4.28  -4.04  -3.8   -3.56  -3.32  -3.08  -2.84  -2.6   -2.36  -2.12]\n",
      "[-13.88 -13.64 -13.4  -13.16 -12.92 -12.68 -12.44 -12.2  -11.96 -11.72\n",
      " -11.48 -11.24 -11.   -10.76 -10.52 -10.28 -10.04  -9.8   -9.56  -9.32\n",
      "  -9.08  -8.84  -8.6   -8.36  -8.12  -7.88  -7.64  -7.4   -7.16  -6.92\n",
      "  -6.68  -6.44  -6.2   -5.96  -5.72  -5.48  -5.24  -5.    -4.76  -4.52\n",
      "  -4.28  -4.04  -3.8   -3.56  -3.32  -3.08  -2.84  -2.6   -2.36  -2.12]\n",
      "Done with SDMass\n"
     ]
    }
   ],
   "source": [
    "# calculate quantities to be stored in obs\n",
    "for obkey,ob in obs.items():\n",
    "    \n",
    "    # calculate observable for GEN, SIM, DATA, and TRUE\n",
    "    ob['genobs'], ob['simobs'] = ob['func'](synthetic, 'gen'), ob['func'](synthetic, 'sim')\n",
    "    ob['truthobs'], ob['dataobs'] = ob['func'](nature, 'gen'), ob['func'](nature, 'sim')\n",
    "    \n",
    "    # setup bins\n",
    "    ob['bins_det'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_det']+1)\n",
    "    ob['bins_mc'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_mc']+1)\n",
    "    ob['midbins_det'] = (ob['bins_det'][:-1] + ob['bins_det'][1:])/2\n",
    "    ob['midbins_mc'] = (ob['bins_mc'][:-1] + ob['bins_mc'][1:])/2\n",
    "\n",
    "    print (ob['midbins_det'])\n",
    "    print (ob['midbins_mc'])\n",
    "    ob['binwidth_det'] = ob['bins_det'][1] - ob['bins_det'][0]\n",
    "    ob['binwidth_mc'] = ob['bins_mc'][1] - ob['bins_mc'][0]\n",
    "    \n",
    "    # get the histograms of GEN, DATA, and TRUTH level observables\n",
    "    ob['genobs_hist'] = np.histogram(ob['genobs'], bins=ob['bins_mc'], density=True)[0]\n",
    "    ob['data_hist'] = np.histogram(ob['dataobs'], bins=ob['bins_det'], density=True)[0]\n",
    "    ob['truth_hist'], ob['truth_hist_unc'] = modplot.calc_hist(ob['truthobs'], bins=ob['bins_mc'], \n",
    "                                                               density=True)[:2]\n",
    "\n",
    "    # compute (and normalize) the response matrix between GEN and SIM\n",
    "    ob['response'] = np.histogram2d(ob['simobs'], ob['genobs'], bins=(ob['bins_det'], ob['bins_mc']))[0]\n",
    "    ob['response'] /= (ob['response'].sum(axis=0) + 10**-50)\n",
    "    \n",
    "    # perform iterative Bayesian unfolding\n",
    "    ob['ibu_phis'] = ibu.ibu(ob['data_hist'], ob['response'], ob['genobs_hist'], \n",
    "                         ob['binwidth_det'], ob['binwidth_mc'], it=itnum)\n",
    "    ob['ibu_phi_unc'] = ibu.ibu_unc(ob, it=itnum, nresamples=25)\n",
    "    \n",
    "    print('Done with', obkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OmniFold\n",
    "\n",
    "Now it's time to set up for the OmniFold procedure and do the unfolding!  \n",
    "\n",
    "Here, we choose model sizes and training parameters that default to a quick training (~5 min). Even with this simplified model and training, we will closely reproduce the full results of the paper.\n",
    "\n",
    "**Customize**: Change the model layer sizes or training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_layer_sizes = [100, 100]\n",
    "# model_layer_sizes = [100, 100, 100] # use this for the full network size\n",
    "\n",
    "# set up the array of data/simulation detector-level observables\n",
    "X_det = np.asarray([np.concatenate((obs[obkey]['dataobs'], obs[obkey]['simobs'])) for obkey in obs_multifold]).T\n",
    "Y_det = ef.utils.to_categorical(np.concatenate((np.ones(len(obs['Mass']['dataobs'])), \n",
    "                                                np.zeros(len(obs['Mass']['simobs'])))))\n",
    "\n",
    "# set up the array of generation particle-level observables\n",
    "X_gen = np.asarray([np.concatenate((obs[obkey]['genobs'], obs[obkey]['genobs'])) for obkey in obs_multifold]).T\n",
    "Y_gen = ef.utils.to_categorical(np.concatenate((np.ones(len(obs['Mass']['genobs'])), \n",
    "                                                np.zeros(len(obs['Mass']['genobs'])))))\n",
    "\n",
    "# standardize the inputs\n",
    "X_det = (X_det - np.mean(X_det, axis=0))/np.std(X_det, axis=0)\n",
    "X_gen = (X_gen - np.mean(X_gen, axis=0))/np.std(X_gen, axis=0)\n",
    "\n",
    "# Specify the training parameters\n",
    "# model parameters for the Step 1 network\n",
    "det_args = {'input_dim': len(obs_multifold), 'dense_sizes': model_layer_sizes,\n",
    "            'patience': 10, 'filepath': 'Step1_{}', 'save_weights_only': False, \n",
    "            'modelcheck_opts': {'save_best_only': True, 'verbose': 1}}\n",
    "\n",
    "# model parameters for the Step 2 network\n",
    "mc_args = {'input_dim': len(obs_multifold), 'dense_sizes': model_layer_sizes, \n",
    "           'patience': 10, 'filepath': 'Step2_{}', 'save_weights_only': False, \n",
    "           'modelcheck_opts': {'save_best_only': True, 'verbose': 1}}\n",
    "\n",
    "# general training parameters\n",
    "fitargs = {'batch_size': 500, 'epochs': 2, 'verbose': 1}\n",
    "#fitargs = {'batch_size': 500, 'epochs': 100, 'verbose': 1} # use this for a full training\n",
    "\n",
    "# reweight the sim and data to have the same total weight to begin with\n",
    "ndata, nsim = np.count_nonzero(Y_det[:,1]), np.count_nonzero(Y_det[:,0])\n",
    "wdata = np.ones(ndata)\n",
    "winit = ndata/nsim*np.ones(nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `omnifold` method within `omnifold.py` takes all the relevant information and performs the unfolding process for the specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 13:09:41.378218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               700       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,002\n",
      "Trainable params: 11,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               700       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,002\n",
      "Trainable params: 11,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 13:09:41.383293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:41.383336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:41.386931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:41.386986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:41.387006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:42.501592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:42.501715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:42.501724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-18 13:09:42.501757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 13:09:42.501791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1565 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 13:10:09.789158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-18 13:10:09.814135: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f66427745d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-18 13:10:09.814191: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2023-12-18 13:10:09.830706: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-18 13:10:09.859477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8800\n",
      "2023-12-18 13:10:09.980925: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-18 13:10:10.097431: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/320 [============================>.] - ETA: 0s - loss: 0.6987 - acc: 0.5574WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.68391, saving model to Step1_0_Epoch-1\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-1/assets\n",
      "320/320 [==============================] - 12s 29ms/step - loss: 0.6984 - acc: 0.5575 - val_loss: 0.6839 - val_acc: 0.5604\n",
      "Epoch 2/2\n",
      "313/320 [============================>.] - ETA: 0s - loss: 0.6827 - acc: 0.5639WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 2: val_loss improved from 0.68391 to 0.68173, saving model to Step1_0_Epoch-2\n",
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-2/assets\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.6826 - acc: 0.5642 - val_loss: 0.6817 - val_acc: 0.5632\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# apply the OmniFold procedure to get weights for the generation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m multifold_ws \u001b[38;5;241m=\u001b[39m \u001b[43momnifold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momnifold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_det\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_det\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwinit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m(\u001b[49m\u001b[43mef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfitargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrw_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/star/unfold/multifold_eg/omnifold.py:129\u001b[0m, in \u001b[0;36momnifold\u001b[0;34m(X_gen_i, Y_gen_i, X_det_i, Y_det_i, wdata, winit, det_model, mc_model, fitargs, val, it, weights_filename, trw_ind, delete_global_arrays)\u001b[0m\n\u001b[1;32m    127\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((ws[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ws[trw_ind]))\n\u001b[1;32m    128\u001b[0m w_train, w_val \u001b[38;5;241m=\u001b[39m w[perm_gen[:\u001b[38;5;241m-\u001b[39mnval_gen]], w[perm_gen[\u001b[38;5;241m-\u001b[39mnval_gen:]]\n\u001b[0;32m--> 129\u001b[0m rw \u001b[38;5;241m=\u001b[39m \u001b[43mreweight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_gen_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_gen_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mc_fp_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfitargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_gen_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_gen_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[invperm_gen]\n\u001b[1;32m    131\u001b[0m ws\u001b[38;5;241m.\u001b[39mappend(rw[\u001b[38;5;28mlen\u001b[39m(ws[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# save the weights if specified\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/star/unfold/multifold_eg/omnifold.py:21\u001b[0m, in \u001b[0;36mreweight\u001b[0;34m(X, Y, w, model, filepath, fitargs, val_data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreweight\u001b[39m(X, Y, w, model, filepath, fitargs, val_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# permute the data, fit the model, and get preditions\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#perm = np.random.permutation(len(X))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#model.fit(X[perm], Y[perm], sample_weight=w[perm], **fitargs)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     val_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_data\u001b[39m\u001b[38;5;124m'\u001b[39m: val_data} \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfitargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mval_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_weights(filepath)\n\u001b[1;32m     23\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39mfitargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m500\u001b[39m))[:,\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/energyflow/archs/archbase.py:370\u001b[0m, in \u001b[0;36mNNBase.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\u001b[38;5;241m.\u001b[39mextend(callbacks)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# do the fitting\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# handle saving at the end, if we weren't already saving throughout \u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_while_training:\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/training.py:1625\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1616\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[1;32m   1617\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\n\u001b[1;32m   1618\u001b[0m         )\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1623\u001b[0m ):\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m-> 1625\u001b[0m     data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/data_adapter.py:1583\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/data_adapter.py:1260\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1259\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/data_adapter.py:252\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    248\u001b[0m     sample_weights, sample_weight_modes\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m (sample_weights, _, _) \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_partial_sample_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight_modes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_all_flat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m inputs \u001b[38;5;241m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[1;32m    258\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mint\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[1;32m    260\u001b[0m )\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/training_utils.py:78\u001b[0m, in \u001b[0;36mhandle_partial_sample_weights\u001b[0;34m(outputs, sample_weights, sample_weight_modes, check_all_flat)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds 1.0 as sample weights for the outputs for which there is no weight.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m  describing the raw sample weights.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m any_sample_weight \u001b[38;5;241m=\u001b[39m sample_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m     76\u001b[0m     w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m sample_weights\n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m partial_sample_weight \u001b[38;5;241m=\u001b[39m any_sample_weight \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_sample_weight:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, any_sample_weight, partial_sample_weight\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/training_utils.py:78\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds 1.0 as sample weights for the outputs for which there is no weight.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m  describing the raw sample weights.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m any_sample_weight \u001b[38;5;241m=\u001b[39m sample_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m     76\u001b[0m     w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m sample_weights\n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m partial_sample_weight \u001b[38;5;241m=\u001b[39m any_sample_weight \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_sample_weight:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, any_sample_weight, partial_sample_weight\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:7374\u001b[0m, in \u001b[0;36m_TensorIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit:\n\u001b[1;32m   7373\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m-> 7374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   7375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   7376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:1084\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1081\u001b[0m   index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;66;03m# stack possibly involves no tensors, so we must use op_scope correct graph.\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_scope\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrided_slice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_on_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m   1088\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m begin:\n\u001b[1;32m   1089\u001b[0m     packed_begin, packed_end, packed_strides \u001b[38;5;241m=\u001b[39m (stack(begin), stack(end),\n\u001b[1;32m   1090\u001b[0m                                                 stack(strides))\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6792\u001b[0m, in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname_scope\u001b[39m(name, default_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skip_on_eager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   6768\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Internal-only entry point for `name_scope*`.\u001b[39;00m\n\u001b[1;32m   6769\u001b[0m \n\u001b[1;32m   6770\u001b[0m \u001b[38;5;124;03m  Internal ops do not use the public API and instead rely on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6790\u001b[0m \u001b[38;5;124;03m    `name_scope*` context manager.\u001b[39;00m\n\u001b[1;32m   6791\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6792\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   6793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m internal_name_scope_v1(name, default_name, values)\n\u001b[1;32m   6795\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m skip_on_eager:\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/context.py:2220\u001b[0m, in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecuting_eagerly\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m():\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether the current thread has eager execution enabled.\u001b[39;00m\n\u001b[1;32m   2169\u001b[0m \n\u001b[1;32m   2170\u001b[0m \u001b[38;5;124;03m  Eager execution is enabled by default and this API returns `True`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;124;03m    `True` if the current thread has eager execution enabled.\u001b[39;00m\n\u001b[1;32m   2219\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2220\u001b[0m   ctx \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2221\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_execution_mode \u001b[38;5;241m==\u001b[39m EAGER_MODE\n",
      "File \u001b[0;32m~/install/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/context.py:2136\u001b[0m, in \u001b[0;36mcontext_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2132\u001b[0m     _create_context()\n\u001b[1;32m   2133\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _context\n\u001b[0;32m-> 2136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontext_safe\u001b[39m():\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns current context (or None if one hasn't been initialized).\"\"\"\u001b[39;00m\n\u001b[1;32m   2138\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _context\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# apply the OmniFold procedure to get weights for the generation\n",
    "multifold_ws = omnifold.omnifold(X_gen, Y_gen, X_det, Y_det, wdata, winit,\n",
    "                                (ef.archs.DNN, det_args), (ef.archs.DNN, mc_args),\n",
    "                                fitargs, val=0.2, it=itnum, trw_ind=-2, weights_filename='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Unfolding Results\n",
    "\n",
    "Now it's time to plot the unfolding results for all of the specified observables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,(obkey,ob) in enumerate(obs.items()):\n",
    "    \n",
    "    # get the styled axes on which to plot\n",
    "    fig, [ax0, ax1] = modplot.axes(**ob)\n",
    "    if ob.get('yscale') is not None:\n",
    "        ax0.set_yscale(ob['yscale'])\n",
    "\n",
    "        \n",
    "    # Plot the Different Distributions of the Observable\n",
    "    # plot the \"data\" histogram of the observable\n",
    "    ax0.hist(ob['dataobs'], bins=ob['bins_det'], color='black', label='``Data\\\"', **hist_style)\n",
    "\n",
    "    # plot the \"sim\" histogram of the observable\n",
    "    ax0.hist(ob['simobs'], bins=ob['bins_det'], color='orange', label='Sim.', **hist_style)\n",
    "\n",
    "    # plot the \"gen\" histogram of the observable\n",
    "    ax0.plot(ob['midbins_mc'], ob['genobs_hist'], **gen_style)\n",
    "\n",
    "    # plot the \"truth\" histogram of the observable\n",
    "    ax0.fill_between(ob['midbins_mc'], ob['truth_hist'], **truth_style)\n",
    "\n",
    "    \n",
    "    # Plot the Unfolded Distributions of the Observable\n",
    "    # plot the OmniFold distribution\n",
    "    of_histgen, of_histgen_unc = modplot.calc_hist(ob['genobs'], weights=multifold_ws[2*itnum], \n",
    "                                                   bins=ob['bins_mc'], density=True)[:2]\n",
    "    ax0.plot(ob['midbins_mc'], of_histgen, **omnifold_style, label='MultiFold')\n",
    "\n",
    "    # plot the IBU distribution\n",
    "    ax0.plot(ob['midbins_mc'], ob['ibu_phis'][itnum], **ibu_style, label='IBU ' + ob['symbol'])\n",
    "\n",
    "    # Plot the Ratios of the OmniFold and IBU distributions to truth (with statistical uncertainties)\n",
    "    ibu_ratio = ob['ibu_phis'][itnum]/(ob['truth_hist'] + 10**-50)\n",
    "    of_ratio = of_histgen/(ob['truth_hist'] + 10**-50)\n",
    "    ax1.plot([np.min(ob['midbins_mc']), np.max(ob['midbins_mc'])], [1, 1], '-', color='green', lw=0.75)\n",
    "    \n",
    "    # ratio uncertainties\n",
    "    truth_unc_ratio = ob['truth_hist_unc']/(ob['truth_hist'] + 10**-50)\n",
    "    ibu_unc_ratio = ob['ibu_phi_unc']/(ob['truth_hist'] + 10**-50)\n",
    "    of_unc_ratio = of_histgen_unc/(ob['truth_hist'] + 10**-50)\n",
    "    \n",
    "    ax1.fill_between(ob['midbins_mc'], 1 - truth_unc_ratio, 1 + truth_unc_ratio, \n",
    "                     facecolor=truth_style['facecolor'], zorder=-2)\n",
    "    ax1.errorbar(ob['midbins_mc'], ibu_ratio, xerr=ob['binwidth_mc']/2, yerr=ibu_unc_ratio, \n",
    "                                              color=ibu_style['color'], **modplot.style('errorbar'))\n",
    "    ax1.errorbar(ob['midbins_mc'], of_ratio, xerr=ob['binwidth_mc']/2, yerr=of_unc_ratio, \n",
    "                                              color=omnifold_style['color'], **modplot.style('errorbar'))\n",
    "\n",
    "    # legend style and ordering\n",
    "    loc, ncol = ob.get('legend_loc', 'upper right'), ob.get('legend_ncol', 2)\n",
    "    order = [3, 4, 2, 5, 0, 1] if ncol==2 else [3, 5, 4, 0, 2, 1]\n",
    "    modplot.legend(ax=ax0, frameon=False, order=order, loc=loc, ncol=ncol)\n",
    "\n",
    "    # stamp to put on the plots\n",
    "    modplot.stamp(*ob['stamp_xy'], delta_y=0.06, ax=ax0,\n",
    "                  line_0=r'\\textbf{D/T}: \\textsc{Herwig 7.1.5} default',\n",
    "                  line_1=r'\\textbf{S/G}: \\textsc{Pythia 8.243} tune 26',\n",
    "                  line_2=r'\\textsc{Delphes 3.4.2} CMS Detector',\n",
    "                  line_3=r'$Z$+jet: $p_T^Z>200$ GeV, $R=0.4$')\n",
    "\n",
    "    # save plot (by default in the same directory as this notebook).\n",
    "    # If running on binder, the plot can be accessed by first going to the jupyter file browser\n",
    "    # (which itself can be accessed by copying the URL of this notebook and removing the name of the notebook\n",
    "    # after the final \"/\"), selecting the square next to the name of the plot, and clicking \"Download\".\n",
    "    fig.savefig('MultiFold_{}.pdf'.format(obkey), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
